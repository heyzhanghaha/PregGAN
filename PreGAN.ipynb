{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import utils\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "from matplotlib.colors import ListedColormap\n",
    "from yellowbrick.classifier import ROCAUC\n",
    "from matplotlib_venn import venn3\n",
    "import matplotlib.patches as mpatches\n",
    "from scipy.stats import normaltest, skew\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import r2_score, median_absolute_error, mean_absolute_error, accuracy_score, f1_score\n",
    "from sklearn.metrics import median_absolute_error, mean_squared_error, mean_squared_log_error\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from scipy.special import boxcox, inv_boxcox\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold, cross_val_predict,  KFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import zscore\n",
    "from itertools import combinations\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import kmapper as km\n",
    "from sklearn.cluster import KMeans\n",
    "from IPython.display import set_matplotlib_formats \n",
    "from torch.autograd import Variable\n",
    "from torch import autograd\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "sns.set_style('whitegrid')\n",
    "sns.set(font_scale=1.5)\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "color= \"Spectral\"\n",
    "color_plt = ListedColormap(sns.color_palette(color).as_hex())\n",
    "color_hist = 'teal'\n",
    "two_colors = [ sns.color_palette(color)[0], sns.color_palette(color)[5]]\n",
    "three_colors = [ sns.color_palette(color)[5],sns.color_palette(color)[2], sns.color_palette(color)[0]]\n",
    "\n",
    "batch_size = 256\n",
    "lr = 0.003\n",
    "ngpu = 1\n",
    "\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "\n",
    "X = pd.read_csv('data/X_metabric_cleaned.csv')\n",
    "y = pd.read_csv('data/y_LS_metabric_cleaned.csv')\n",
    "\n",
    "X1, X_test1, Y1, Y_test1 = train_test_split(X, y, test_size=0.20, random_state=42, stratify = y)\n",
    "\n",
    "X = np.array(X1)\n",
    "Y = np.array(Y1)\n",
    "X_test = np.array(X_test1)\n",
    "Y_test = np.array(Y_test1)\n",
    "\n",
    "X=torch.from_numpy(X).float() \n",
    "Y=torch.from_numpy(Y).float()\n",
    "X_test=torch.from_numpy(X_test).float()\n",
    "Y_test=torch.from_numpy(Y_test).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (main): Sequential(\n",
      "    (0): Linear(in_features=50, out_features=64, bias=True)\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (7): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): Linear(in_features=64, out_features=1, bias=True)\n",
      "    (10): Sigmoid()\n",
      "  )\n",
      ")\n",
      "Discriminator(\n",
      "  (l1): Linear(in_features=31, out_features=64, bias=True)\n",
      "  (i1): InstanceNorm1d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "  (r1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (l2): Linear(in_features=64, out_features=128, bias=True)\n",
      "  (i2): InstanceNorm1d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "  (r2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (l3): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (i3): InstanceNorm1d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "  (r3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (l4): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__ (self,ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(50, 64, bias = True),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(True),\n",
    "        \n",
    "            nn.Linear(64,128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.Linear(128,64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.Linear(64,1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def weight_init(self, mean, std):\n",
    "        for m in self._modules:\n",
    "            normal_init(self._modules[m], mean, std)\n",
    "        \n",
    "    def forward(self,input):\n",
    "        return self.main(input)\n",
    "    \n",
    "netG = Generator(ngpu).to(device)\n",
    "if (device.type == 'cuda') and (ngpu > 1):netG = nn.DataParallel(netG, list(range(ngpu)))\n",
    "print(netG)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__ (self,ngpu):\n",
    "        self.ngpu = ngpu\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.l1=nn.Linear(in_features = 31,out_features = 64,bias = True)\n",
    "        self.i1=nn.InstanceNorm1d(64)\n",
    "        self.r1=nn.LeakyReLU(0.2, inplace=True)\n",
    "        \n",
    "        self.l2=nn.Linear(in_features = 64,out_features = 128,bias = True)\n",
    "        self.i2=nn.InstanceNorm1d(128)\n",
    "        self.r2=nn.LeakyReLU(0.2, inplace=True)\n",
    "            \n",
    "        self.l3=nn.Linear(in_features = 128,out_features = 64,bias = True)\n",
    "        self.i3=nn.InstanceNorm1d(64)\n",
    "        self.r3=nn.LeakyReLU(0.2, inplace=True)\n",
    "           \n",
    "        self.l4=nn.Linear(64,1)\n",
    "\n",
    "    def weight_init(self, mean, std):\n",
    "        for m in self._modules:\n",
    "            normal_init(self._modules[m], mean, std)\n",
    "        \n",
    "    def forward(self,input):\n",
    "        out=self.l1(input)\n",
    "        out=out.reshape(256,1,64)\n",
    "        out=self.i1(out)\n",
    "        out=out.reshape(256,64)\n",
    "        out=self.r1(out)\n",
    "        \n",
    "        out=self.l2(out)\n",
    "        out=out.reshape(256,1,128)\n",
    "        out=self.i2(out)\n",
    "        out=out.reshape(256,128)\n",
    "        out=self.r2(out)\n",
    "\n",
    "        out=self.l3(out)\n",
    "        out=out.reshape(256,1,64)\n",
    "        out=self.i3(out)\n",
    "        out=out.reshape(256,64)\n",
    "        out=self.r3(out)\n",
    "        \n",
    "        out=self.l4(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "def normal_init(m, mean, std):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        m.weight.data.normal_(mean, std)\n",
    "        m.bias.data.zero_()\n",
    "        \n",
    "netD = Discriminator(ngpu).to(device)\n",
    "if (device.type == 'cuda') and (ngpu > 1):netD = nn.DataParallel(netD, list(range(ngpu)))\n",
    "print(netD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netG.weight_init(mean=0, std=0.02)\n",
    "netD.weight_init(mean=0, std=0.02)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr,betas=(0.5, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr,betas=(0.5, 0.999))\n",
    "\n",
    "def calculate_gradient_penalty(real_images, fake_images):\n",
    "    eta = torch.FloatTensor(batch_size,1).uniform_(0,1)\n",
    "    eta = eta.expand(batch_size, real_images.size(1))\n",
    "    if device:\n",
    "        eta = eta.to(device)\n",
    "    else:\n",
    "        eta = eta\n",
    "\n",
    "    interpolated = eta * real_images + ((1 - eta) * fake_images)\n",
    "\n",
    "    if device:\n",
    "        interpolated = interpolated.to(device)\n",
    "    else:\n",
    "        interpolated = interpolated\n",
    "\n",
    "    # define it to calculate gradient\n",
    "    interpolated = Variable(interpolated, requires_grad=True)\n",
    "\n",
    "    # calculate probability of interpolated examples\n",
    "    prob_interpolated = netD(interpolated)\n",
    "\n",
    "    # calculate gradients of probabilities with respect to examples\n",
    "    gradients = autograd.grad(outputs=prob_interpolated, inputs=interpolated,\n",
    "                            grad_outputs=torch.ones(\n",
    "                                prob_interpolated.size()).to(device) if device else torch.ones(\n",
    "                                prob_interpolated.size()),\n",
    "                            create_graph=True, retain_graph=True)[0]\n",
    "\n",
    "    grad_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * 10\n",
    "    return grad_penalty\n",
    "\n",
    "torch.manual_seed(1368)\n",
    "rs = np.random.RandomState(1368)\n",
    "\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "\n",
    "print(\"Starting Training Loop...\")\n",
    "for epoch in range(10000):\n",
    "    for _ in range(5):\n",
    "        netD.zero_grad()\n",
    "            \n",
    "        idx = rs.choice(X.shape[0],batch_size)\n",
    "        x_data=X[idx]\n",
    "        y_data=Y[idx]\n",
    "        \n",
    "        c = torch.cat((x_data,y_data),dim=1).to(device)\n",
    "        d_real = netD(c)\n",
    "        \n",
    "        d_real_loss = criterion(d_real,torch.full_like(d_real, 1))\n",
    "        d_real_loss.backward()\n",
    "        \n",
    "        noise = torch.randn(len(x_data), 20)\n",
    "\n",
    "        y = x_data.to(device)\n",
    "        y = torch.cat((noise,x_data),dim=1).to(device)\n",
    "\n",
    "        fake = netG(y).detach()\n",
    "        x_data=x_data.to(device)\n",
    "        \n",
    "        z = torch.cat((x_data,fake),dim=1).to(device)\n",
    "        \n",
    "        d_fake = netD(z)\n",
    "        d_fake_loss = criterion(d_fake,torch.full_like(d_fake, 0))\n",
    "        d_fake_loss.backward()\n",
    "\n",
    "        gradient_penalty = calculate_gradient_penalty(c.data, fake.data)\n",
    "        gradient_penalty.backward()\n",
    "\n",
    "        d_loss = d_fake_loss - d_real_loss + gradient_penalty\n",
    "        Wasserstein_D = d_real_loss - d_fake_loss\n",
    "        \n",
    "        optimizerD.step()\n",
    "        \n",
    "    idx = rs.choice(X.shape[0],batch_size)\n",
    "    x_data=X[idx]\n",
    "    y_data=Y[idx]\n",
    "            \n",
    "    netG.zero_grad()\n",
    "\n",
    "    noise = torch.randn(len(x_data), 20)\n",
    "    a = x_data.to(device)\n",
    "    a = torch.cat((noise,x_data),dim=1).to(device)\n",
    "    fake = netG(a)\n",
    "    x_data=x_data.to(device)\n",
    "\n",
    "    b = torch.cat((x_data,fake),dim=1).to(device)\n",
    "    \n",
    "    d_g = netD(b)\n",
    "        \n",
    "    g_loss = criterion(d_g, torch.full_like(d_g, 1))\n",
    "        \n",
    "    g_loss.backward()\n",
    "    optimizerG.step()\n",
    "        \n",
    "    if epoch % 50 == 0:\n",
    "        print('[%d]\\tLoss_D: %.4f\\tLoss_G: %.4f'\n",
    "                % (epoch,d_loss.item(), g_loss.item()))\n",
    "\n",
    "    G_losses.append(g_loss.item())\n",
    "    D_losses.append(d_loss.item())\n",
    "    \n",
    "torch.save(netG, 'data\\G_parameter.pkl')\n",
    "torch.save(netD, 'data\\D_parameter.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
